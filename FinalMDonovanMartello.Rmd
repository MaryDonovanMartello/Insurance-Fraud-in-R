---
title: "Final Project:  Auto Claim Fraud"
author: "Mary Donovan Martello"
date: "5/28/2020"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyr)
library(ggplot2)
library(pastecs)
library(ggm)
library(dplyr)
library(psych)
library(VIM)
library(plyr)
library(QuantPsyc)
library(car)
library(caTools)
library(class)
library(caret)
library(ltm)

```

## The following is the initial data set for this project:


```{r claims}
# import data
claims_with_Outlier = read.csv("FradulentInsuranceClaims.csv")
str(claims_with_Outlier)
```

## Data Cleaning

In my exploratory data analysis, I used simple histograms and counts to look for outliers.  None of the histograms identified any outliers.  However, the counts() for umbrella_limit shows an obvious outlier of a negative 1 million:

```{r}
count(claims_with_Outlier$umbrella_limit)


```

I filtered out the negative 1 million observation for the intial data set and named the dataframe object "claims":


```{r}
# Filter to remove the outliers
claims <- claims_with_Outlier %>% 
    filter(umbrella_limit >= 0)

```

My Final Report includes my discussion on data exploration and data cleaning.  This is the code for the data cleaning:


```{r}
# Remove empty variable/column
claims <- Filter(function(x)!all(is.na(x)), claims)

# separate policy_csl variable into two variables and convert them to int data type
claims <- separate(claims, "policy_csl", into=c("cslBodily","cslProp"), sep="/")
claims$cslBodily <- as.integer(claims$cslBodily)
claims$cslProp <- as.integer(claims$cslProp)

#convert two variables to date data type
claims$incident_date <- as.Date(claims$incident_date)
claims$policy_bind_date <- as.Date(claims$policy_bind_date)

# add a new variable to hold calculation of the difference of weeks between to date variables
claims$weeks_bf_incident<- difftime(claims$incident_date,claims$policy_bind_date, units = c("weeks"))

# remove two columns that produced the new weeks_bf_incident variable from the dataframe
claims$incident_date <- NULL
claims$policy_bind_date <- NULL

# convert months to weeks in another variable to be consistent with weeks_bf_incident variable
claims$months_as_customer <- claims$months_as_customer * 4.33

# change name of variable to reflect that it is now measured in weeks
names(claims)[names(claims) == "months_as_customer"] <- "weeks_as_customer"

# three variables have "?" instead of NA; replace ? with one of the other factors
claims$collision_type <- gsub("?","Rear Collision",claims$collision_type, fixed = TRUE)
claims$property_damage <- gsub("?","YES",claims$property_damage, fixed = TRUE)
claims$police_report_available <- gsub("?","NO",claims$police_report_available, fixed = TRUE)

# convert the data type back to factors 
claims$collision_type <- as.factor(claims$collision_type)
claims$property_damage <- as.factor(claims$property_damage)
claims$police_report_available <- as.factor(claims$police_report_available)


```

## Categorical Variables 

In order to make use of the several factored variables in the data set, I needed to convert the factor type variables to numeric type variables.  However, for some of the analysis I still needed the categorical variables to be factor data types.  Thus, instead of converting these categorical variables back and forth between factors and numeric I created a copy of the dataframe and converted the categorical variables to numeric in the copied dataframe:


```{r}
# make copy of dataframe to use numeric categorical variables
claimsNum <- claims

# add variables for converted factor to numeric
claimsNum$policy_stateNum <- as.numeric(claims$policy_state)
claimsNum$insured_sexNum <- as.numeric(claims$insured_sex)
claimsNum$insured_education_levelNum <- as.numeric(claims$insured_education_level)
claimsNum$insured_occupationNum <- as.numeric(claims$insured_occupation)
claimsNum$insured_hobbiesNum <- as.numeric(claims$insured_hobbies)
claimsNum$insured_relationshipNum <- as.numeric(claims$insured_relationship)
claimsNum$incident_typeNum <- as.numeric(claims$incident_type)
claimsNum$collision_typeNum <- as.numeric(claims$collision_type)
claimsNum$incident_severityNum <- as.numeric(claims$incident_severity)
claimsNum$authorities_contactedNum <- as.numeric(claims$authorities_contacted)
claimsNum$incident_stateNum <- as.numeric(claims$incident_state)
claimsNum$incident_cityNum <- as.numeric(claims$incident_city)
claimsNum$property_damageNum <- as.numeric(claims$property_damage)
claimsNum$police_report_availableNum <- as.numeric(claims$police_report_available)
claimsNum$auto_makeNum <- as.numeric(claims$auto_make)
claimsNum$auto_modelNum <- as.numeric(claims$auto_model)
claimsNum$fraud_reportedNum <- as.numeric(claims$fraud_reported)
claimsNum$weeks_bf_incident <- as.numeric(claimsNum$weeks_bf_incident)
```

The following is the clean dataframe with converted categorical variables:

```{r}
str(claimsNum)

```


## Fraud Reported Subsets

I created subsets of both dataframes based on the fraud_reported variable.  The fraud_reported variable is either Y or N to indicate if a fraud was reported on the auto insurance claim transaction.  The subsets are useful for identifying differences in the data and thus identifying variables influencing fraud:

```{r}
# Subsets 
noFraudSubsection <- subset(claims, fraud_reported == "N")
yesFraudSubsection <- subset(claims, fraud_reported == "Y")
noFraudSubsectionNum <- subset(claimsNum, fraud_reported == "N")
yesFraudSubsectionNum <- subset(claimsNum, fraud_reported == "Y")
```

## Histograms to Identify Influencing Variables

I used histograms with normal curves and colored by the fraud_reported variable to look for variables that might be an indication of a fraudulent auto claim.  Many of the histograms showed the same curve for the fraud_reported variable, and thus did not appear to identify a difference for fraudulent claims.  However, some of the histograms show a difference in the curve for fraud_reported = Y versus fraud_reported = N.  These may indicate variables that can identify fraudulent claims.

The fraud_reported == "N" subset has 752 observations but the fraud_reported == "Y" subset has only 247 observations.  If each subset had the same proportional number of values for a variable, then the histogram's red curve (for N) would always be proportionally higher than the green curve (for Y).  However, for some variables the green curve (for Y) was much higher than the red curve.  The following histograms for incident_severity, total_claim_amount and weeks_bf_incident demonstrate this discrepancy.

```{r}
# Histogram of the incident_severityNum   variable with normal curve
ggplot(claimsNum, aes(x=incident_severityNum, col = fraud_reported)) + 
    geom_histogram(binwidth = 1, aes(y=..density..), colour="black", fill="white") +
    geom_density(alpha=.2, fill="yellow") +
    labs(title="Incident Severity Histogram and Curve", x="v", y = "Count")
```


```{r}
ggplot(claims, aes(x=total_claim_amount, col = fraud_reported)) + 
    geom_histogram(binwidth = 1000, aes(y=..density..), colour="black", fill="white") +
    geom_density(alpha=.2, fill="yellow") +
    labs(title="Total Claim Amount Histogram and Curve", x="total_claim_amount", y = "Count")
```


```{r}
ggplot(claims, aes(x=weeks_bf_incident, col = fraud_reported)) + 
    geom_histogram(binwidth = 50, aes(y=..density..), colour="black", fill="white") +
    geom_density(alpha=.2, fill="yellow") +
    labs(title="Weeks Before Incident Histogram and Curve", x="weeks_bf_incident", y = "Count")
```

## Correlation

My outcome variable is fraud_reported.  In order to determine which variables will explain this outcome variable, I ran correlation functions to try to discover some correlation between fraud_reported and other variables.  Fraud_reported is a discrete dichotomous variable.  Point-biserial correlation is used when one of the variables is a discrete dichotomous variable.  The point-biserial correlation is equivalent to calculating the Pearson correlation between a continuous and a dichotomous variable. Therefore, I can use the standard cor.test function in R, which will output the correlation, a 95% confidence interval, and an independent t-test with associated p-value.

The p-values from the correlation test is the significance level of the t-test.  If the p-value is less than 5% then we can conclude that the correlation is significant.  The variables that have p-values that are 6% or less are total_claim_amount, incident_severity, and umbrella_limit.

```{r}
cor.test(claimsNum$total_claim_amount, claimsNum$fraud_reportedNum, method = "pearson")
```


```{r}
cor.test(claimsNum$umbrella_limit, claimsNum$fraud_reportedNum, method = "pearson")
```


```{r}
cor.test(claimsNum$incident_severityNum, claimsNum$fraud_reportedNum, method = "pearson")
```

Given the results of the correlation tests, I ran partial correlation on the fraud_reported and incident-severity variables, with total_claim_amount and umbrella_limit as controlled variables.  These correlations are stronger than any of the two-variable correlations.

```{r}

```


```{r}
claimsPC1 <- claimsNum[, c("fraud_reportedNum", "total_claim_amount", "incident_severityNum")]
pc <- pcor(c("fraud_reportedNum", "incident_severityNum", "total_claim_amount"), var(claimsPC1))
pc
```


```{r}
claimsPC2 <- claimsNum[, c("fraud_reportedNum", "umbrella_limit", "incident_severityNum")]
pc <- pcor(c("fraud_reportedNum", "incident_severityNum", "umbrella_limit"), var(claimsPC2))
pc
```


```{r}
claimsPC3 <- claimsNum[, c("fraud_reportedNum", "umbrella_limit", "total_claim_amount", "incident_severityNum")]
pc <- pcor(c("fraud_reportedNum", "incident_severityNum", "umbrella_limit", "total_claim_amount"), var(claimsPC3))
pc
```

## Logistic Regression

My outcome variable is a binary categorical variable and therefore I am using logistic regression to try to determine explanatory variables to predict the fraud_reported variable.  Note:  I am able to use the categorical variables in the regression model as I converted them to numeric data types.

I first included all of the variables in the formula with the exception of (1) the three variables with a "?" as a value because I have no means of deciding on meaningful replacement values and (2) the variables that caused multicollinearity.  There was a multicollinearity problem with including all the variables.  I tested for multicollinearity with the vif() function.  When the vif() function shows a multicollinearity problem, you can see which variables are the cause of the problem with the alias() function.  I removed the variables causing the multicollinearity problem and the three "?" variables, and ran a logistic regression model with the remaining variables (the "Permissible Variables Model").  

The accuracy result of the Permissible Variables Model is 80.58%:

```{r}
claimsMR10 <- claimsNum[, c(1:3, 5:10, 16:17, 25:26, 28:29, 31:31, 34:34, 37:46, 48:51, 54:55)]
logModel <- glm(fraud_reported ~ ., data = claimsMR10, family = binomial(), maxit = 100)

# make predictions
predlogModel <- predict(logModel, type = "response")

# make confusion matrix
confMatrix <- table(Actual_value = claimsNum$fraud_reported, 
                    Predicted_Value = predlogModel > 0.50)
confMatrix

# calculate accuracy
(confMatrix[[1,1]] + confMatrix[[2,2]]) / sum(confMatrix) * 100
```

z-statistics for Permissible Variables Model:

```{r}
summary(logModel)
```

Odds ratio for Permissible Variables Model:

```{r}
# calculate odds ratio
exp(coef(logModel))
```

Next, I tried to improve on the accuracy of the logistic regression model by using only some of the Permissible Variables in the formula.  I reviewed the odds ratio from the Permissible Variable Model and only included the variables that had an odds ratio of 1 or greater.  However, the accuracy from this model decreased to 75.18%.  

Because incident_severity showed to be significant in the counts, histogram and correlation analysis I added incident_severity back to the model even though it only showed an odds ratio of 0.273.  The accuracy results of this model (the "Odds Ratio + Severity Model") is 81.48%:

```{r}
claimsMR8 <- claimsNum[, c(38:38, 1:1, 5:5, 7:7, 9:10, 20:20, 28:29, 34:34, 40:43, 45:46, 49:49)]
logModel <- glm(fraud_reported ~ ., data = claimsMR8, family = binomial(), maxit = 100)
# make predictions
predlogModel <- predict(logModel, type = "response")

# make confusion matrix
confMatrix <- table(Actual_value = claimsNum$fraud_reported, 
                    Predicted_Value = predlogModel > 0.50)
confMatrix

# calculate accuracy
(confMatrix[[1,1]] + confMatrix[[2,2]]) / sum(confMatrix) * 100
```

I tried various other combinations of variables for the logistic regression model and the highest accuracy I found is the 81.48% for the "Odds Ratio + Severity Model".

## Classification Model

Classification algorithms can be used predict categorical outcomes.  My research question is ultimately determining the outcome of the categorical fraud_reported variable and thus I am using a k nearest neighbor model to try to predict the fraud_reported variable.  

Because the dataframe has many categorical variables, I used one-hot encoding to convert the factors to dummy variables so that they may be used in the training data and test data in the machine learning model.

The following is the code to create dummy variables for the categorical variables:

```{r}
# Converting every categorical variable to numerical using dummy variables
dummy <- dummyVars(" ~ .", data=claims, fullRank = T)
claimsDummy <- data.frame(predict(dummy, newdata = claims))
# Converting the dependent variable back to categorical 
claimsDummy$fraud_reported.Y <- as.factor(claimsDummy$fraud_reported.Y)
```

The classification model I am using is the k nearest neighbor model.  The following fits a k nearest neighbor model for the claimsDummy data set:

```{r}

```

```{r}
set.seed(123)
# data splicing
claimsDummyOutcome <- claimsDummy[, c("fraud_reported.Y")]
split <- sample.split(claimsDummyOutcome, SplitRatio = 0.65)
train <- subset(claimsDummy, split == "TRUE")
test <- subset(claimsDummy, split == "FALSE")
# 144 is the outcome variable
train_claims <- claimsDummy[split == "TRUE", 144]
test_claims <- claimsDummy[split == "FALSE", 144]
# knn model
knn.roundup <- knn(train=train, test=test, cl=train_claims, k=5)
#Calculate the proportion of correct classification 
ACC.roundup <- 100 * sum(test_claims == knn.roundup)/NROW(test_claims)
ACC.roundup


# Check prediction against actual value in tabular form 
confMatrixUp <- table(knn.roundup, test_claims)
confMatrixUp

```

However, please note that the accuracy result and confusion matrix that displays in the Rmd file (above) does not match the accuracy result and confusion matrix in the R file.  The accuarcy result for this same code was 71.06017% and the confusion matrix was as follows:

```{r}
#              test_claims
#knn.roundup     0     1
#          0   238    76
#          1    25    10
```

I have tried to figure out why the Rmd displays different results from the R file in the console of RStudio, but I cannot fix it.  The Rmd results shown above appear incorrect because they do not show the last line of the confusion matrix and has an accuracy result of 99%+, which is very much higher than anything RStudio is producing in any of the scenarios that I ran.  

```{r}

```


```{r}

```



